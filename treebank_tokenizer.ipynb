{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "treebank_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FShIojWKOAyi"
      },
      "source": [
        "# Treebank Word Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-HuLXK__d5Ee",
        "outputId": "c1aafc51-a293-4b47-e022-556776135155"
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iexGvZqyd9gJ",
        "outputId": "8aacc39a-83df-4366-e6ac-3bbda828418b"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "eXBs7WfBcx6-",
        "outputId": "57880d7b-73db-4e2f-81a7-141093fb0fa8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train-DelDupHy.csv')\n",
        "df"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3786</td>\n",
              "      <td>3</td>\n",
              "      <td>-- but certainly hard to hate .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7154</td>\n",
              "      <td>4</td>\n",
              "      <td>-- but it makes for one of the most purely enj...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8867</td>\n",
              "      <td>3</td>\n",
              "      <td>I also wanted a little alien as a friend !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6938</td>\n",
              "      <td>1</td>\n",
              "      <td>-- is a crime that should be punishable by cha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3110</td>\n",
              "      <td>0</td>\n",
              "      <td>spy action flick with Antonio Banderas and L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7650</th>\n",
              "      <td>7650</td>\n",
              "      <td>8217</td>\n",
              "      <td>2</td>\n",
              "      <td>Ze movie starts out so funny , then she is not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7651</th>\n",
              "      <td>7651</td>\n",
              "      <td>5083</td>\n",
              "      <td>1</td>\n",
              "      <td>Zellweger 's whiny pouty lipped poof faced and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7652</th>\n",
              "      <td>7652</td>\n",
              "      <td>776</td>\n",
              "      <td>3</td>\n",
              "      <td>Zhuangzhuang creates delicate balance of style...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7653</th>\n",
              "      <td>7653</td>\n",
              "      <td>4976</td>\n",
              "      <td>1</td>\n",
              "      <td>ZigZag might have been richer and more observa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7654</th>\n",
              "      <td>7654</td>\n",
              "      <td>1263</td>\n",
              "      <td>3</td>\n",
              "      <td>Zoom !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7655 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                           Sentence\n",
              "0              0  ...                    -- but certainly hard to hate .\n",
              "1              1  ...  -- but it makes for one of the most purely enj...\n",
              "2              2  ...         I also wanted a little alien as a friend !\n",
              "3              3  ...  -- is a crime that should be punishable by cha...\n",
              "4              4  ...    spy action flick with Antonio Banderas and L...\n",
              "...          ...  ...                                                ...\n",
              "7650        7650  ...  Ze movie starts out so funny , then she is not...\n",
              "7651        7651  ...  Zellweger 's whiny pouty lipped poof faced and...\n",
              "7652        7652  ...  Zhuangzhuang creates delicate balance of style...\n",
              "7653        7653  ...  ZigZag might have been richer and more observa...\n",
              "7654        7654  ...                                             Zoom !\n",
              "\n",
              "[7655 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW83z-0ZdEi4",
        "outputId": "09e0f5c7-765a-4d9f-8563-28c3d892a4f9"
      },
      "source": [
        "sentences = list(df['Sentence'])\n",
        "sentences[:4]\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "vocab_count_tree = {}\n",
        "token_array_tree = []\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "token_array_check = [tokenizer.tokenize(s) for s in sentences]\n",
        "\n",
        "# sentence chunking, token 빈도 counting\n",
        "for i in sentences:\n",
        "  chunk_sentence = tokenizer.tokenize(i)\n",
        "  result = []\n",
        "  for token in chunk_sentence:\n",
        "    result.append(token)\n",
        "    if token not in vocab_count_tree:\n",
        "      vocab_count_tree[token] = 0\n",
        "    vocab_count_tree[token] += 1\n",
        "  token_array_tree.append(result)\n",
        "\n",
        "# token를 빈도수로 mapping 배열한 list\n",
        "vocab_sorted_tree = sorted(vocab_count_tree.items(), key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# vocab 파일 생성\n",
        "vocab_tree = [i[0] for i in vocab_sorted_tree]\n",
        "with open('./vocab_DelDupHy_tree.txt', 'w+') as lf:\n",
        "    lf.write('\\n'.join(vocab_tree))\n",
        "\n",
        "# token를 빈도순으로 Indexing\n",
        "token_index_tree = [tuple([w, i+1]) for i, w in enumerate(vocab_tree)]\n",
        "\n",
        "\n",
        "print('Found %s unique tokens' % len(vocab_tree))\n",
        "print('After chunking \\t\\t\\t:', token_array_tree[:5])\n",
        "print('Vocab in frequency order \\t:',vocab_tree[:20])\n",
        "print('List of (token, frequency) \\t:', vocab_sorted_tree[:20])\n",
        "print('List of (token, index) \\t\\t:', token_index_tree[:20])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16223 unique tokens\n",
            "After chunking \t\t\t: [['--', 'but', 'certainly', 'hard', 'to', 'hate', '.'], ['--', 'but', 'it', 'makes', 'for', 'one', 'of', 'the', 'most', 'purely', 'enjoyable', 'and', 'satisfying', 'evenings', 'at', 'the', 'movies', 'I', \"'ve\", 'had', 'in', 'a', 'while', '.'], ['I', 'also', 'wanted', 'a', 'little', 'alien', 'as', 'a', 'friend', '!'], ['--', 'is', 'a', 'crime', 'that', 'should', 'be', 'punishable', 'by', 'chainsaw', '.'], ['spy', 'action', 'flick', 'with', 'Antonio', 'Banderas', 'and', 'Lucy', 'Liu', 'never', 'comes', 'together', '.']]\n",
            "Vocab in frequency order \t: ['.', ',', 'the', 'and', 'of', 'a', 'to', 'is', \"'s\", 'that', 'in', 'it', 'The', 'as', 'film', 'with', 'but', 'movie', 'for', 'its']\n",
            "List of (token, frequency) \t: [('.', 7187), (',', 6408), ('the', 5471), ('and', 4004), ('of', 3993), ('a', 3958), ('to', 2694), ('is', 2279), (\"'s\", 2264), ('that', 1692), ('in', 1624), ('it', 1569), ('The', 1142), ('as', 1079), ('film', 1027), ('with', 956), ('but', 952), ('movie', 895), ('for', 873), ('its', 831)]\n",
            "List of (token, index) \t\t: [('.', 1), (',', 2), ('the', 3), ('and', 4), ('of', 5), ('a', 6), ('to', 7), ('is', 8), (\"'s\", 9), ('that', 10), ('in', 11), ('it', 12), ('The', 13), ('as', 14), ('film', 15), ('with', 16), ('but', 17), ('movie', 18), ('for', 19), ('its', 20)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5ffwCHMdynV",
        "outputId": "7683772a-274b-43b7-cb1a-f8568b3c27cf"
      },
      "source": [
        "token_array_check == token_array_tree"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp32wN7Mg52T"
      },
      "source": [
        "# With refined data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "2Mh4q1VmezZ_",
        "outputId": "cbbeb269-399b-4e6d-f614-5ce5cfe40f8c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_sent.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11180</td>\n",
              "      <td>2</td>\n",
              "      <td>Although Barbershop boasts some of today's hot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10282</td>\n",
              "      <td>1</td>\n",
              "      <td>-LRB- Johnnie To and Wai Ka Fai are -RRB- sure...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1049</td>\n",
              "      <td>1</td>\n",
              "      <td>Naipaul fans may be disappointed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10985</td>\n",
              "      <td>4</td>\n",
              "      <td>Narc is a no-bull throwback to 1970s action fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4235</td>\n",
              "      <td>1</td>\n",
              "      <td>Cinematic pyrotechnics aside , the only thing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9807</th>\n",
              "      <td>5209</td>\n",
              "      <td>0</td>\n",
              "      <td>Horrible.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9808</th>\n",
              "      <td>3227</td>\n",
              "      <td>2</td>\n",
              "      <td>The Country Bears has no scenes that will upse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9809</th>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>... the picture's cleverness is ironically mut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9810</th>\n",
              "      <td>7451</td>\n",
              "      <td>3</td>\n",
              "      <td>It gives devastating testimony to both people'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9811</th>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>This is the best Star Trek movie in a long time.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9812 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Id  Category                                           Sentence\n",
              "0     11180         2  Although Barbershop boasts some of today's hot...\n",
              "1     10282         1  -LRB- Johnnie To and Wai Ka Fai are -RRB- sure...\n",
              "2      1049         1                  Naipaul fans may be disappointed.\n",
              "3     10985         4  Narc is a no-bull throwback to 1970s action fi...\n",
              "4      4235         1  Cinematic pyrotechnics aside , the only thing ...\n",
              "...     ...       ...                                                ...\n",
              "9807   5209         0                                          Horrible.\n",
              "9808   3227         2  The Country Bears has no scenes that will upse...\n",
              "9809   2016         2  ... the picture's cleverness is ironically mut...\n",
              "9810   7451         3  It gives devastating testimony to both people'...\n",
              "9811     36         4   This is the best Star Trek movie in a long time.\n",
              "\n",
              "[9812 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-bUF8zZh0S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbe7a39-7597-4273-fe7f-eb69711d1868"
      },
      "source": [
        "sentences = list(df['Sentence'])\n",
        "sentences[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Although Barbershop boasts some of today's hottest and hippest acts from the world of television , music and stand-up comedy , this movie strangely enough has the outdated swagger of a shameless ` 70s blaxploitation shuck-and-jive sitcom.\",\n",
              " \"-LRB- Johnnie To and Wai Ka Fai are -RRB- sure to find an enthusiastic audience among American action-adventure buffs , but the film's interests may be too narrow to attract crossover viewers.\",\n",
              " 'Naipaul fans may be disappointed.',\n",
              " 'Narc is a no-bull throwback to 1970s action films.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PqarOx6hB94",
        "outputId": "542147cf-85ca-4f30-979c-825356932fe6"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "vocab_count_tree = {}\n",
        "token_array_tree = []\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "token_array_check = [tokenizer.tokenize(s) for s in sentences]\n",
        "\n",
        "# sentence chunking, token 빈도 counting\n",
        "for i in sentences:\n",
        "  chunk_sentence = tokenizer.tokenize(i)\n",
        "  result = []\n",
        "  for token in chunk_sentence:\n",
        "    result.append(token)\n",
        "    if token not in vocab_count_tree:\n",
        "      vocab_count_tree[token] = 0\n",
        "    vocab_count_tree[token] += 1\n",
        "  token_array_tree.append(result)\n",
        "\n",
        "# token를 빈도수로 mapping 배열한 list\n",
        "vocab_sorted_tree = sorted(vocab_count_tree.items(), key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# vocab 파일 생성\n",
        "vocab_tree = [i[0] for i in vocab_sorted_tree]\n",
        "with open('./vocab_tree_ref.txt', 'w+') as lf:\n",
        "    lf.write('\\n'.join(vocab_tree))\n",
        "\n",
        "# token를 빈도순으로 Indexing\n",
        "token_index_tree = [tuple([w, i+1]) for i, w in enumerate(vocab_tree)]\n",
        "\n",
        "\n",
        "print('Found %s unique tokens' % len(vocab_tree))\n",
        "print('After chunking \\t\\t\\t:', token_array_tree[:5])\n",
        "print('Vocab in frequency order \\t:',vocab_tree[:20])\n",
        "print('List of (token, frequency) \\t:', vocab_sorted_tree[:20])\n",
        "print('List of (token, index) \\t\\t:', token_index_tree[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17235 unique tokens\n",
            "After chunking \t\t\t: [['Although', 'Barbershop', 'boasts', 'some', 'of', 'today', \"'s\", 'hottest', 'and', 'hippest', 'acts', 'from', 'the', 'world', 'of', 'television', ',', 'music', 'and', 'stand-up', 'comedy', ',', 'this', 'movie', 'strangely', 'enough', 'has', 'the', 'outdated', 'swagger', 'of', 'a', 'shameless', '`', '70s', 'blaxploitation', 'shuck-and-jive', 'sitcom', '.'], ['-LRB-', 'Johnnie', 'To', 'and', 'Wai', 'Ka', 'Fai', 'are', '-RRB-', 'sure', 'to', 'find', 'an', 'enthusiastic', 'audience', 'among', 'American', 'action-adventure', 'buffs', ',', 'but', 'the', 'film', \"'s\", 'interests', 'may', 'be', 'too', 'narrow', 'to', 'attract', 'crossover', 'viewers', '.'], ['Naipaul', 'fans', 'may', 'be', 'disappointed', '.'], ['Narc', 'is', 'a', 'no-bull', 'throwback', 'to', '1970s', 'action', 'films', '.'], ['Cinematic', 'pyrotechnics', 'aside', ',', 'the', 'only', 'thing', 'Avary', 'seems', 'to', 'care', 'about', 'are', 'mean', 'giggles', 'and', 'pulchritude', '.']]\n",
            "Vocab in frequency order \t: ['.', ',', 'the', 'a', 'and', 'of', 'to', 'is', \"'s\", 'that', 'in', 'it', 'The', 'as', 'film', 'but', 'with', 'movie', 'for', 'its']\n",
            "List of (token, frequency) \t: [('.', 9196), (',', 8163), ('the', 6913), ('a', 5079), ('and', 5058), ('of', 5030), ('to', 3404), ('is', 2923), (\"'s\", 2872), ('that', 2174), ('in', 2065), ('it', 2011), ('The', 1459), ('as', 1400), ('film', 1293), ('but', 1234), ('with', 1225), ('movie', 1110), ('for', 1107), ('its', 1082)]\n",
            "List of (token, index) \t\t: [('.', 1), (',', 2), ('the', 3), ('a', 4), ('and', 5), ('of', 6), ('to', 7), ('is', 8), (\"'s\", 9), ('that', 10), ('in', 11), ('it', 12), ('The', 13), ('as', 14), ('film', 15), ('but', 16), ('with', 17), ('movie', 18), ('for', 19), ('its', 20)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16NjEmDihu-e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}