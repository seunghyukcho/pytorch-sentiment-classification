{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkwGVrMRSRYe"
      },
      "source": [
        "# NLTK sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "zbEwjY6uHh3g",
        "outputId": "38f3a62f-bbea-4b14-d214-724637cb3d07"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "df"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11180</td>\n",
              "      <td>2</td>\n",
              "      <td>Although Barbershop boasts some of today 's ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10282</td>\n",
              "      <td>1</td>\n",
              "      <td>-LRB- Johnnie To and Wai Ka Fai are -RRB- sure...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1049</td>\n",
              "      <td>1</td>\n",
              "      <td>Naipaul fans may be disappointed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10985</td>\n",
              "      <td>4</td>\n",
              "      <td>Narc is a no-bull throwback to 1970s action fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4235</td>\n",
              "      <td>1</td>\n",
              "      <td>Cinematic pyrotechnics aside , the only thing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9807</th>\n",
              "      <td>5209</td>\n",
              "      <td>0</td>\n",
              "      <td>Horrible .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9808</th>\n",
              "      <td>3227</td>\n",
              "      <td>2</td>\n",
              "      <td>The Country Bears has no scenes that will upse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9809</th>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>... the picture 's cleverness is ironically mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9810</th>\n",
              "      <td>7451</td>\n",
              "      <td>3</td>\n",
              "      <td>It gives devastating testimony to both people ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9811</th>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>This is the best Star Trek movie in a long time .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9812 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Id  Category                                           Sentence\n",
              "0     11180         2  Although Barbershop boasts some of today 's ho...\n",
              "1     10282         1  -LRB- Johnnie To and Wai Ka Fai are -RRB- sure...\n",
              "2      1049         1                 Naipaul fans may be disappointed .\n",
              "3     10985         4  Narc is a no-bull throwback to 1970s action fi...\n",
              "4      4235         1  Cinematic pyrotechnics aside , the only thing ...\n",
              "...     ...       ...                                                ...\n",
              "9807   5209         0                                         Horrible .\n",
              "9808   3227         2  The Country Bears has no scenes that will upse...\n",
              "9809   2016         2  ... the picture 's cleverness is ironically mu...\n",
              "9810   7451         3  It gives devastating testimony to both people ...\n",
              "9811     36         4  This is the best Star Trek movie in a long time .\n",
              "\n",
              "[9812 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfw8sTMJH1HQ",
        "outputId": "379d2222-170c-4aae-a3a0-d5cb05c01d30"
      },
      "source": [
        "sentences = list(df['Sentence'])\n",
        "sentences[:4]\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "vocab_count_sent = {}\n",
        "token_array_sent = []\n",
        "token_array_check = [sent_tokenize(s) for s in sentences]\n",
        "\n",
        "# sentence chunking, token 빈도 counting\n",
        "for i in sentences:\n",
        "  sent_token = sent_tokenize(i)\n",
        "  chunk_sentence = i.split(' and ')  # 추가된 기준으로 chunking\n",
        "  result = []\n",
        "  for token in chunk_sentence:\n",
        "    result.append(token)\n",
        "    if token not in vocab_count_sent:\n",
        "      vocab_count_sent[token] = 0\n",
        "    vocab_count_sent[token] += 1\n",
        "  token_array_sent.append(result)\n",
        "\n",
        "# token를 빈도수로 mapping 배열한 list\n",
        "vocab_sorted_sent = sorted(vocab_count_sent.items(), key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# vocab 파일 생성\n",
        "vocab_sent = [i[0] for i in vocab_sorted_sent]\n",
        "with open('./vocab_crude_Sent_and.txt', 'w+') as lf:\n",
        "    lf.write('\\n'.join(vocab_sent))\n",
        "\n",
        "# token를 빈도순으로 Indexing\n",
        "token_index_sent = [tuple([w, i+1]) for i, w in enumerate(vocab_sent)]\n",
        "\n",
        "\n",
        "print('Found %s unique tokens' % len(vocab_sent))\n",
        "print('After chunking \\t\\t\\t:', token_array_sent[:5])\n",
        "print('Vocab in frequency order \\t:',vocab_sent[:20])\n",
        "print('List of (token, frequency) \\t:', vocab_sorted_sent[:20])\n",
        "print('List of (token, index) \\t\\t:', token_index_sent[:20])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11555 unique tokens\n",
            "After chunking \t\t\t: [[\"Although Barbershop boasts some of today 's hottest\", 'hippest acts from the world of television , music', 'stand-up comedy , this movie strangely enough has the outdated swagger of a shameless ` 70s blaxploitation shuck-and-jive sitcom .'], ['-LRB- Johnnie To', \"Wai Ka Fai are -RRB- sure to find an enthusiastic audience among American action-adventure buffs , but the film 's interests may be too narrow to attract crossover viewers .\"], ['Naipaul fans may be disappointed .'], ['Narc is a no-bull throwback to 1970s action films .'], ['Cinematic pyrotechnics aside , the only thing Avary seems to care about are mean giggles', 'pulchritude .']]\n",
            "Vocab in frequency order \t: ['A moving', 'funny .', 'worth a look .', 'tedious .', 'fear .', 'artificial .', 'mayhem .', 'Deliberately', 'far between .', 'isolation .', 'Poignant', 'humor', 'amateurish .', 'Smart', \"What 's next ?\", 'why .', 'touching .', \"For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped .\", 'Funny', 'over again .']\n",
            "List of (token, frequency) \t: [('A moving', 7), ('funny .', 5), ('worth a look .', 4), ('tedious .', 4), ('fear .', 4), ('artificial .', 4), ('mayhem .', 4), ('Deliberately', 4), ('far between .', 4), ('isolation .', 3), ('Poignant', 3), ('humor', 3), ('amateurish .', 3), ('Smart', 3), (\"What 's next ?\", 3), ('why .', 3), ('touching .', 3), (\"For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped .\", 3), ('Funny', 3), ('over again .', 3)]\n",
            "List of (token, index) \t\t: [('A moving', 1), ('funny .', 2), ('worth a look .', 3), ('tedious .', 4), ('fear .', 5), ('artificial .', 6), ('mayhem .', 7), ('Deliberately', 8), ('far between .', 9), ('isolation .', 10), ('Poignant', 11), ('humor', 12), ('amateurish .', 13), ('Smart', 14), (\"What 's next ?\", 15), ('why .', 16), ('touching .', 17), (\"For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped .\", 18), ('Funny', 19), ('over again .', 20)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7-zjFiJIJGrZ",
        "outputId": "663d79ae-c0ce-491c-ab95-e12223fb1b2e"
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dof5s7bJLWi",
        "outputId": "4b03883a-02f7-414d-be6e-0fab18c63c68"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtgQ_BL4H5sg",
        "outputId": "b9fe92fc-bdf9-4c56-d8a3-5a3654986096"
      },
      "source": [
        "sentences = list(df['Sentence'])\n",
        "sentences[:4]\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "vocab_count_sent = {}\n",
        "token_array_sent = []\n",
        "token_array_check = [sent_tokenize(s) for s in sentences]\n",
        "\n",
        "# sentence chunking, token 빈도 counting\n",
        "for i in sentences:\n",
        "  sent_token = sent_tokenize(i)\n",
        "  chunk_sentence = i.split(' and ')  # 추가된 기준으로 chunking\n",
        "  result = []\n",
        "  for token in chunk_sentence:\n",
        "    result.append(token)\n",
        "    if token not in vocab_count_sent:\n",
        "      vocab_count_sent[token] = 0\n",
        "    vocab_count_sent[token] += 1\n",
        "  token_array_sent.append(result)\n",
        "\n",
        "# token를 빈도수로 mapping 배열한 list\n",
        "vocab_sorted_sent = sorted(vocab_count_sent.items(), key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# vocab 파일 생성\n",
        "vocab_sent = [i[0] for i in vocab_sorted_sent]\n",
        "with open('./vocab_ref21_Sent_and.txt', 'w+') as lf:\n",
        "    lf.write('\\n'.join(vocab_sent))\n",
        "\n",
        "# token를 빈도순으로 Indexing\n",
        "token_index_sent = [tuple([w, i+1]) for i, w in enumerate(vocab_sent)]\n",
        "\n",
        "\n",
        "print('Found %s unique tokens' % len(vocab_sent))\n",
        "print('After chunking \\t\\t\\t:', token_array_sent[:5])\n",
        "print('Vocab in frequency order \\t:',vocab_sent[:20])\n",
        "print('List of (token, frequency) \\t:', vocab_sorted_sent[:20])\n",
        "print('List of (token, index) \\t\\t:', token_index_sent[:20])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11555 unique tokens\n",
            "After chunking \t\t\t: [[\"Although Barbershop boasts some of today 's hottest\", 'hippest acts from the world of television , music', 'stand-up comedy , this movie strangely enough has the outdated swagger of a shameless ` 70s blaxploitation shuck-and-jive sitcom .'], ['-LRB- Johnnie To', \"Wai Ka Fai are -RRB- sure to find an enthusiastic audience among American action-adventure buffs , but the film 's interests may be too narrow to attract crossover viewers .\"], ['Naipaul fans may be disappointed .'], ['Narc is a no-bull throwback to 1970s action films .'], ['Cinematic pyrotechnics aside , the only thing Avary seems to care about are mean giggles', 'pulchritude .']]\n",
            "Vocab in frequency order \t: ['A moving', 'funny .', 'worth a look .', 'tedious .', 'fear .', 'artificial .', 'mayhem .', 'Deliberately', 'far between .', 'isolation .', 'Poignant', 'humor', 'amateurish .', 'Smart', \"What 's next ?\", 'why .', 'touching .', \"For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped .\", 'Funny', 'over again .']\n",
            "List of (token, frequency) \t: [('A moving', 7), ('funny .', 5), ('worth a look .', 4), ('tedious .', 4), ('fear .', 4), ('artificial .', 4), ('mayhem .', 4), ('Deliberately', 4), ('far between .', 4), ('isolation .', 3), ('Poignant', 3), ('humor', 3), ('amateurish .', 3), ('Smart', 3), (\"What 's next ?\", 3), ('why .', 3), ('touching .', 3), (\"For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped .\", 3), ('Funny', 3), ('over again .', 3)]\n",
            "List of (token, index) \t\t: [('A moving', 1), ('funny .', 2), ('worth a look .', 3), ('tedious .', 4), ('fear .', 5), ('artificial .', 6), ('mayhem .', 7), ('Deliberately', 8), ('far between .', 9), ('isolation .', 10), ('Poignant', 11), ('humor', 12), ('amateurish .', 13), ('Smart', 14), (\"What 's next ?\", 15), ('why .', 16), ('touching .', 17), (\"For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped .\", 18), ('Funny', 19), ('over again .', 20)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFEMD6vnKOHr"
      },
      "source": [
        "# Issues\n",
        "data refining\n",
        " - (space)', (space)n't, (space)., (space)?, (space)!, \n",
        " - (space),\n",
        " - (space)'\n",
        " - Mr.(space)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIgrjkleL_PN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "ba838c0c-57c1-4727-875b-7cd1c986a0a0"
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11180</td>\n",
              "      <td>2</td>\n",
              "      <td>Although Barbershop boasts some of today 's ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10282</td>\n",
              "      <td>1</td>\n",
              "      <td>-LRB- Johnnie To and Wai Ka Fai are -RRB- sure...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1049</td>\n",
              "      <td>1</td>\n",
              "      <td>Naipaul fans may be disappointed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10985</td>\n",
              "      <td>4</td>\n",
              "      <td>Narc is a no-bull throwback to 1970s action fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4235</td>\n",
              "      <td>1</td>\n",
              "      <td>Cinematic pyrotechnics aside , the only thing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9807</th>\n",
              "      <td>5209</td>\n",
              "      <td>0</td>\n",
              "      <td>Horrible .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9808</th>\n",
              "      <td>3227</td>\n",
              "      <td>2</td>\n",
              "      <td>The Country Bears has no scenes that will upse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9809</th>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>... the picture 's cleverness is ironically mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9810</th>\n",
              "      <td>7451</td>\n",
              "      <td>3</td>\n",
              "      <td>It gives devastating testimony to both people ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9811</th>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>This is the best Star Trek movie in a long time .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9812 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Id  Category                                           Sentence\n",
              "0     11180         2  Although Barbershop boasts some of today 's ho...\n",
              "1     10282         1  -LRB- Johnnie To and Wai Ka Fai are -RRB- sure...\n",
              "2      1049         1                 Naipaul fans may be disappointed .\n",
              "3     10985         4  Narc is a no-bull throwback to 1970s action fi...\n",
              "4      4235         1  Cinematic pyrotechnics aside , the only thing ...\n",
              "...     ...       ...                                                ...\n",
              "9807   5209         0                                         Horrible .\n",
              "9808   3227         2  The Country Bears has no scenes that will upse...\n",
              "9809   2016         2  ... the picture 's cleverness is ironically mu...\n",
              "9810   7451         3  It gives devastating testimony to both people ...\n",
              "9811     36         4  This is the best Star Trek movie in a long time .\n",
              "\n",
              "[9812 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FsxrmIXJCi2",
        "outputId": "6e16eca8-4754-471d-d6ea-6d684100b87e"
      },
      "source": [
        "sentences = list(df['Sentence'])\n",
        "sentences[:4]\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "vocab_count_sent = {}\n",
        "token_array_sent = []\n",
        "token_array_check = [sent_tokenize(s) for s in sentences]\n",
        "\n",
        "# sentence chunking, token 빈도 counting\n",
        "for i in sentences:\n",
        "  sent_token = sent_tokenize(i)\n",
        "  chunk_sentence = i.split(',')  # 쉼표 기준으로 chunking\n",
        "  result = []\n",
        "  for token in chunk_sentence:\n",
        "    result.append(token)\n",
        "    if token not in vocab_count_sent:\n",
        "      vocab_count_sent[token] = 0\n",
        "    vocab_count_sent[token] += 1\n",
        "  token_array_sent.append(result)\n",
        "\n",
        "# token를 빈도수로 mapping 배열한 list\n",
        "vocab_sorted_sent = sorted(vocab_count_sent.items(), key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# vocab 파일 생성\n",
        "vocab_sent = [i[0] for i in vocab_sorted_sent]\n",
        "with open('./vocab_CrudeSent_comma.txt', 'w+') as lf:\n",
        "    lf.write('\\n'.join(vocab_sent))\n",
        "\n",
        "# token를 빈도순으로 Indexing\n",
        "token_index_sent = [tuple([w, i+1]) for i, w in enumerate(vocab_sent)]\n",
        "\n",
        "\n",
        "print('Found %s unique tokens' % len(vocab_sent))\n",
        "print('After chunking \\t\\t\\t:', token_array_sent[:5])\n",
        "print('Vocab in frequency order \\t:',vocab_sent[:20])\n",
        "print('List of (token, frequency) \\t:', vocab_sorted_sent[:20])\n",
        "print('List of (token, index) \\t\\t:', token_index_sent[:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13819 unique tokens\n",
            "After chunking \t\t\t: [[\"Although Barbershop boasts some of today 's hottest and hippest acts from the world of television \", ' music and stand-up comedy ', ' this movie strangely enough has the outdated swagger of a shameless ` 70s blaxploitation shuck-and-jive sitcom .'], ['-LRB- Johnnie To and Wai Ka Fai are -RRB- sure to find an enthusiastic audience among American action-adventure buffs ', \" but the film 's interests may be too narrow to attract crossover viewers .\"], ['Naipaul fans may be disappointed .'], ['Narc is a no-bull throwback to 1970s action films .'], ['Cinematic pyrotechnics aside ', ' the only thing Avary seems to care about are mean giggles and pulchritude .']]\n",
            "Vocab in frequency order \t: [' however ', ' though ', 'Unfortunately ', 'In the end ', ' but ', ' and ', ' really ', 'Well ', ' either .', 'Yes ', 'Ultimately ', 'A dull ', ' in the end ', 'Sadly ', 'Overall ', ' yes ', 'A loud ', ' tap ', 'All in all ', 'A sensitive ']\n",
            "List of (token, frequency) \t: [(' however ', 15), (' though ', 14), ('Unfortunately ', 12), ('In the end ', 10), (' but ', 9), (' and ', 7), (' really ', 7), ('Well ', 7), (' either .', 7), ('Yes ', 7), ('Ultimately ', 7), ('A dull ', 6), (' in the end ', 6), ('Sadly ', 6), ('Overall ', 6), (' yes ', 6), ('A loud ', 6), (' tap ', 6), ('All in all ', 5), ('A sensitive ', 5)]\n",
            "List of (token, index) \t\t: [(' however ', 1), (' though ', 2), ('Unfortunately ', 3), ('In the end ', 4), (' but ', 5), (' and ', 6), (' really ', 7), ('Well ', 8), (' either .', 9), ('Yes ', 10), ('Ultimately ', 11), ('A dull ', 12), (' in the end ', 13), ('Sadly ', 14), ('Overall ', 15), (' yes ', 16), ('A loud ', 17), (' tap ', 18), ('All in all ', 19), ('A sensitive ', 20)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VguKUTQgvO7I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6JOVcKpEtuy"
      },
      "source": [
        "# NLTK word tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "iktWKP6MFi09",
        "outputId": "5e6c778c-a8f2-41c0-ba7e-79b8ef50356e"
      },
      "source": [
        "df = pd.read_csv('train-DelDupHy.csv')\n",
        "df"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3786</td>\n",
              "      <td>3</td>\n",
              "      <td>-- but certainly hard to hate .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7154</td>\n",
              "      <td>4</td>\n",
              "      <td>-- but it makes for one of the most purely enj...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8867</td>\n",
              "      <td>3</td>\n",
              "      <td>I also wanted a little alien as a friend !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6938</td>\n",
              "      <td>1</td>\n",
              "      <td>-- is a crime that should be punishable by cha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3110</td>\n",
              "      <td>0</td>\n",
              "      <td>spy action flick with Antonio Banderas and L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7650</th>\n",
              "      <td>7650</td>\n",
              "      <td>8217</td>\n",
              "      <td>2</td>\n",
              "      <td>Ze movie starts out so funny , then she is not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7651</th>\n",
              "      <td>7651</td>\n",
              "      <td>5083</td>\n",
              "      <td>1</td>\n",
              "      <td>Zellweger 's whiny pouty lipped poof faced and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7652</th>\n",
              "      <td>7652</td>\n",
              "      <td>776</td>\n",
              "      <td>3</td>\n",
              "      <td>Zhuangzhuang creates delicate balance of style...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7653</th>\n",
              "      <td>7653</td>\n",
              "      <td>4976</td>\n",
              "      <td>1</td>\n",
              "      <td>ZigZag might have been richer and more observa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7654</th>\n",
              "      <td>7654</td>\n",
              "      <td>1263</td>\n",
              "      <td>3</td>\n",
              "      <td>Zoom !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7655 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                           Sentence\n",
              "0              0  ...                    -- but certainly hard to hate .\n",
              "1              1  ...  -- but it makes for one of the most purely enj...\n",
              "2              2  ...         I also wanted a little alien as a friend !\n",
              "3              3  ...  -- is a crime that should be punishable by cha...\n",
              "4              4  ...    spy action flick with Antonio Banderas and L...\n",
              "...          ...  ...                                                ...\n",
              "7650        7650  ...  Ze movie starts out so funny , then she is not...\n",
              "7651        7651  ...  Zellweger 's whiny pouty lipped poof faced and...\n",
              "7652        7652  ...  Zhuangzhuang creates delicate balance of style...\n",
              "7653        7653  ...  ZigZag might have been richer and more observa...\n",
              "7654        7654  ...                                             Zoom !\n",
              "\n",
              "[7655 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvlSwnRmbsPo",
        "outputId": "ef9a11a2-9554-45bd-9e2b-e404410431e2"
      },
      "source": [
        "sentences = list(df['Sentence'])\n",
        "sentences[:4]\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "vocab_count_word = {}\n",
        "token_array_word = []\n",
        "token_array_check = [word_tokenize(s) for s in sentences]\n",
        "\n",
        "# sentence chunking, token 빈도 counting\n",
        "for i in sentences:\n",
        "  chunk_sentence = word_tokenize(i)\n",
        "  result = []\n",
        "  for token in chunk_sentence:\n",
        "    result.append(token)\n",
        "    if token not in vocab_count_word:\n",
        "      vocab_count_word[token] = 0\n",
        "    vocab_count_word[token] += 1\n",
        "  token_array_word.append(result)\n",
        "\n",
        "# token를 빈도수로 mapping 배열한 list\n",
        "vocab_sorted_word = sorted(vocab_count_word.items(), key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# vocab 파일 생성\n",
        "vocab_word = [i[0] for i in vocab_sorted_word]\n",
        "with open('./vocab_DelDupHy_word.txt', 'w+') as lf:\n",
        "    lf.write('\\n'.join(vocab_word))\n",
        "\n",
        "# token를 빈도순으로 Indexing\n",
        "token_index_word = [tuple([w, i+1]) for i, w in enumerate(vocab_word)]\n",
        "\n",
        "\n",
        "print('Found %s unique tokens' % len(vocab_word))\n",
        "print('After chunking \\t\\t\\t:', token_array_word[:5])\n",
        "print('Vocab in frequency order \\t:',vocab_word[:20])\n",
        "print('List of (token, frequency) \\t:', vocab_sorted_word[:20])\n",
        "print('List of (token, index) \\t\\t:', token_index_word[:20])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16221 unique tokens\n",
            "After chunking \t\t\t: [['--', 'but', 'certainly', 'hard', 'to', 'hate', '.'], ['--', 'but', 'it', 'makes', 'for', 'one', 'of', 'the', 'most', 'purely', 'enjoyable', 'and', 'satisfying', 'evenings', 'at', 'the', 'movies', 'I', \"'ve\", 'had', 'in', 'a', 'while', '.'], ['I', 'also', 'wanted', 'a', 'little', 'alien', 'as', 'a', 'friend', '!'], ['--', 'is', 'a', 'crime', 'that', 'should', 'be', 'punishable', 'by', 'chainsaw', '.'], ['spy', 'action', 'flick', 'with', 'Antonio', 'Banderas', 'and', 'Lucy', 'Liu', 'never', 'comes', 'together', '.']]\n",
            "Vocab in frequency order \t: ['.', ',', 'the', 'and', 'of', 'a', 'to', 'is', \"'s\", 'that', 'in', 'it', 'The', 'as', 'film', 'with', 'but', 'movie', 'for', 'its']\n",
            "List of (token, frequency) \t: [('.', 7236), (',', 6408), ('the', 5471), ('and', 4004), ('of', 3993), ('a', 3958), ('to', 2694), ('is', 2279), (\"'s\", 2264), ('that', 1692), ('in', 1624), ('it', 1569), ('The', 1142), ('as', 1079), ('film', 1027), ('with', 956), ('but', 952), ('movie', 895), ('for', 873), ('its', 831)]\n",
            "List of (token, index) \t\t: [('.', 1), (',', 2), ('the', 3), ('and', 4), ('of', 5), ('a', 6), ('to', 7), ('is', 8), (\"'s\", 9), ('that', 10), ('in', 11), ('it', 12), ('The', 13), ('as', 14), ('film', 15), ('with', 16), ('but', 17), ('movie', 18), ('for', 19), ('its', 20)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbxqYjLncUDd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}